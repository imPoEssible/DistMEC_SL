{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/DistMEC\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/DistMEC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from classes.User import *\n",
    "from classes.solver import *\n",
    "from classes.Server import *\n",
    "from classes.distributed_utils import *\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from time import sleep\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoT_User(User):\n",
    "    \n",
    "    def __init__(self, locs, svr_locs, mu, idx, \n",
    "                 max_dist = 7, threshold_dist = 6, self_weight = 0.5, P = None,\n",
    "                 c1 = 100, c2 = 600, c3 = 600, delta = 0, rho = 0.5, epsilon = 0.01,\n",
    "                 c = None, horizon = 15000):\n",
    "        \"\"\"\n",
    "        ceiling = max number of reservation time step\n",
    "        sticky_mode = stick with same arm for set number of time steps once reserved\n",
    "        kick_mode = other user with higher production can interupt reservation when collision\n",
    "        \"\"\"\n",
    "        # max dist - reward range\n",
    "        # threshold dist - used for generating markov chain\n",
    "        \n",
    "        self.idx = idx\n",
    "        self.locs = locs\n",
    "        self.dists = self.get_dists()\n",
    "        self.svr_locs = svr_locs\n",
    "        self.mu = mu # True weights\n",
    "        \n",
    "        self.t = 0 # Time-steps past\n",
    "        self.mode = \"blind\" # oracle\n",
    "        self.phase = 0 #{0-exploration,1-GoT,2-exploitation}\n",
    "        self.t_p = 0 #{timestep within phase }\n",
    "        self.k = 0 # epoch\n",
    "        \n",
    "        self.c = [c1,c2,c3]\n",
    "        self.horizon = horizon\n",
    "        \n",
    "        self.delta = delta\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        if c == None:\n",
    "            self.c = len(svr_locs)\n",
    "        self.got_base_arm = 0\n",
    "        \n",
    "        \n",
    "        if P is None:\n",
    "            self.P = self.make_P(threshold_dist, self_weight)\n",
    "        else:\n",
    "            self.P = P\n",
    "            \n",
    "        self.reward_dists = self.get_reward_dists()\n",
    "        self.reward_scale = self.get_scales(max_dist)\n",
    "        self.usr_place = self.init_loc()\n",
    "        \n",
    "        self.stationary_loc = self.get_stationary_loc()\n",
    "        self.stationary_reward_scale = self.get_stationary_scale()\n",
    "        \n",
    "        # Initialize learning parameters\n",
    "        self.pulls = np.zeros(len(svr_locs))\n",
    "        self.param_summed = np.zeros(len(svr_locs))\n",
    "        self.Ftni = np.zeros(len(svr_locs)) # number of rounds in content state\n",
    "        self.Fmax_idx = 0 # arm to pull during exploitation phase\n",
    "        self.mu_est = np.zeros(len(svr_locs))\n",
    "        self.state = 'discontent'\n",
    "        \n",
    "        self.epoch_time_mapping = np.zeros(horizon)\n",
    "        self.phase_time_mapping = np.zeros(horizon)\n",
    "        \n",
    "        self.set_epochs()\n",
    "\n",
    "        \n",
    "        # history\n",
    "        self.history_location = []\n",
    "        self.history_pull = []\n",
    "        self.history_reward = []\n",
    "        self.history_collisions = []\n",
    "        \n",
    "    def set_epochs(self):\n",
    "        \n",
    "        pass_flag = True\n",
    "        curr_phase = 0\n",
    "        t_track = 0\n",
    "        temp_k = 1\n",
    "        \n",
    "        while pass_flag:\n",
    "            if curr_phase == 0:\n",
    "                num_steps = self.c[curr_phase]\n",
    "            elif curr_phase == 1:\n",
    "                num_steps = self.c[curr_phase] * (temp_k)**(1+self.delta)\n",
    "            elif curr_phase == 2:\n",
    "                num_steps = self.c[curr_phase] * 2**(temp_k)\n",
    "            \n",
    "            end_time = t_track + num_steps\n",
    "            \n",
    "            if t_track + num_steps >= self.horizon:\n",
    "                end_time = self.horizon\n",
    "                pass_flag = False\n",
    "            \n",
    "            self.epoch_time_mapping[t_track:end_time] = np.ones(num_steps)*(temp_k) \n",
    "            self.phase_time_mapping[t_track:end_time] = np.ones(num_steps) * curr_phase\n",
    "            \n",
    "            curr_phase += 1\n",
    "            if curr_phase > 2:\n",
    "                curr_phase = 0\n",
    "                temp_k += 1\n",
    "            \n",
    "            return\n",
    "            \n",
    "        \n",
    "    def update_mean(self):\n",
    "        \"\"\"\n",
    "        Update decision variables for next round\n",
    "        \"\"\"\n",
    "\n",
    "        reward_record = self.param_summed\n",
    "        pulls_record = self.pulls\n",
    "        \n",
    "        for s in range(reward_record.shape[0]):\n",
    "            if pulls_record[s] > 0:\n",
    "                mean = reward_record[s]/pulls_record[s]\n",
    "                self.mu_est[s] = mean\n",
    "    \n",
    "    def choose_arm(self):\n",
    "        # Choose an arm to pull based on collision restriction and UCB info\n",
    "        \n",
    "        if phase == 0: # Exploration Phase\n",
    "            arm_id = np.random.randint(low=0, high=len(self.svr_locs))\n",
    "        elif phase == 1: # GoT Phase\n",
    "            if self.state == 'content':\n",
    "                sub_arm_prob = self.epsilon**self.c / (len(self.svr_locs)-1)\n",
    "                pdf = np.ones(len(self.svr_locs)) * sub_arm_prob\n",
    "                pdf[self.got_base_arm] = 1 - self.epsilon**c\n",
    "                arm_id = np.random.choice(len(self.svr_locs, 1, p=pdf))\n",
    "            elif self.state == 'discontent':\n",
    "                arm_id = np.random.randint(low=0, high=len(self.svr_locs))\n",
    "        elif phase == 2: # Exploitation Phase\n",
    "            arm_id = self.Fmax_idx\n",
    "        \n",
    "        return arm_id\n",
    "    \n",
    "    def receive_reward(self, arm_id, reward, collision_flag, max_reward, wait_time, chosen_idx,\n",
    "                       reservation_mode = False):\n",
    "\n",
    "        scale = self.stationary_reward_scale[arm_id]\n",
    "        \n",
    "        if phase == 0: # Exploration Phase                              \n",
    "            if not collision_flag:\n",
    "                self.pulls[arm_id] += 1\n",
    "                self.param_summed[arm_id] += reward/scale\n",
    "                self.update_mean()\n",
    "        elif phase == 1: # GoT Phase\n",
    "            if arm_id == self.got_base_arm and not collision flag and self.state == 'content':\n",
    "                pass\n",
    "            else:\n",
    "                self.got_base_arm = arm_id\n",
    "                uns = self.stationary_reward_scale* self.mu_est\n",
    "                un_max = np.max(uns)\n",
    "                un = uns[arm_id] * (1-collision_flag)\n",
    "                prob_c = (un/un_max)*self.epsilon**(un_max-un)\n",
    "                if np.random.binomial(n=1,p=prob_c):\n",
    "                    self.state == 'content'\n",
    "                    if self.t_p > np.ceil(self.rho*self.c2*self.k**(1+self.delta)):\n",
    "                        self.Ftni[self.got_base_arm] += 1\n",
    "                else:\n",
    "                    self.state == 'discontent'\n",
    "        elif phase == 2: # Exploitation Phase\n",
    "            pass\n",
    "        \n",
    "            \n",
    "        # Update history\n",
    "        self.history_location += [self.usr_place]\n",
    "        self.history_pull += [arm_id]\n",
    "        self.history_reward += [reward]\n",
    "        self.history_collisions += [collision_flag]\n",
    "        \n",
    "        self.advance_time()\n",
    "        \n",
    "    def advance_time(self):\n",
    "        self.t += 1\n",
    "        self.t_p += 1\n",
    "        \n",
    "        try:\n",
    "            prev_epoch = self.epoch_time_mapping[t-1]\n",
    "            now_epoch = self.epoch_time_mapping[t]\n",
    "            prev_phase = self.phase_time_mapping[t-1]\n",
    "            now_phase = self.phase_time_mapping[t]\n",
    "            \n",
    "            if prev_epoch != now_epoch: # Reset all epoch (2-->0)\n",
    "                self.t_p = 0\n",
    "                self.state = 'discontent'\n",
    "                self.Ftni = np.zeros(len(svr_locs)) # number of rounds in content state\n",
    "                self.Fmax_idx = 0\n",
    "                \n",
    "            elif prev_phase != now_phase: # Go to next phase\n",
    "                self.t_p = 0\n",
    "                \n",
    "                if now_phase == 2:\n",
    "                    self.Fmax_idx = np.argmax(self.Ftni) \n",
    "        except:\n",
    "            pass\n",
    "            print(\"advance time pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "if True and True and not True:\n",
    "    print(1)\n",
    "else: print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array([4,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
