{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tkim/Academics/DistMEC\n"
     ]
    }
   ],
   "source": [
    "cd /home/tkim/Academics/DistMEC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from classes.User import *\n",
    "from classes.solver import *\n",
    "from classes.Server import *\n",
    "from classes.distributed_utils import *\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from time import sleep\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_reward_collisions(Users,T):\n",
    "    save_array = np.zeros([len(Users),T])\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        for u in range(len(Users)):\n",
    "            save_array[u,t] = (1 - Users[u].history_collisions[t]) * np.sum(Users[u].history_reward[t])\n",
    "    \n",
    "    return save_array\n",
    "\n",
    "def extract_total_train_count(Users_w_sa):\n",
    "    U, T = save_array.shape\n",
    "    rwd = 0\n",
    "    \n",
    "    np.sum(Users_w_sa > 0.5,axis=0)\n",
    "    \n",
    "    np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Parameters\n",
    "T = 1000\n",
    "num_users = 16\n",
    "num_svrs = 16\n",
    "num_svrs_loc = 16\n",
    "num_locs = 9\n",
    "mu = np.random.uniform(low=0, high = 1, size = [num_users, num_svrs])\n",
    "ceiling = 40\n",
    "# w = np.random.uniform(low=0, high=1, size= [num_users, num_svrs])\n",
    "space_1d_dim = 10 #4 for small, 10 for big\n",
    "svr_offset_uniform = 1\n",
    "max_dist = 4.0\n",
    "\n",
    "num_explore = 1\n",
    "trials = 1\n",
    "\n",
    "# result dictionaries\n",
    "result_dicts = []\n",
    "performance_list = []\n",
    "user_dicts = []\n",
    "\n",
    "save_name = 'results/22_06_16_ceiling_20/'\n",
    "isExist = os.path.exists(save_name)\n",
    "if not isExist:\n",
    "  # Create a new directory because it does not exist \n",
    "    os.makedirs(save_name)\n",
    "\n",
    "\n",
    "# Generate visitng locations and server locations\n",
    "usr_locs = gen_eq_locs(space_1d=space_1d_dim, nums=num_locs,offset = 1.7)\n",
    "\n",
    "with open('traces_P_40k.pkl', 'rb') as handle:\n",
    "    P_collection = pickle.load(handle)\n",
    "    \n",
    "with open('traces_P_40k_dict.pkl', 'rb') as handle:\n",
    "    P_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b102c042b44cecad276dabcc6c8978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=983), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Recording Mechanism\n",
    "reward_worst_dict = {}\n",
    "collisions_worst_dict = {}\n",
    "\n",
    "reward_rsv_dict = {}\n",
    "collisions_rsv_dict = {}\n",
    "\n",
    "reward_opt_stat = {}\n",
    "\n",
    "    \n",
    "for tri in range(trials):\n",
    "    svr_locs = gen_rand_locs(space_1d=space_1d_dim - svr_offset_uniform, nums=num_svrs_loc)\n",
    "    # Create Users\n",
    "    Users_w = []\n",
    "    for i in range(num_users):\n",
    "        P_temp = P_collection\n",
    "        Users_w += [User(usr_locs,svr_locs,mu[i],i,\n",
    "                      max_dist = max_dist, threshold_dist = 6, self_weight = 1 - 0.85, P = P_temp, ceiling = 1,\n",
    "                      sticky_mode = True, kick_mode=True)]\n",
    "\n",
    "    GoT_Users = []\n",
    "    for i in range(num_users):\n",
    "        GoT_Users += [GoT_User(usr_locs, svr_locs, mu[i], i, \n",
    "                     max_dist = max_dist, threshold_dist = 6, self_weight = 1 - 0.95, P = Users_w[i].P,\n",
    "                     c1 = 5000, c2 = 6000, c3 = 6000, delta = 0, rho = 0.5, epsilon = 0.1,\n",
    "                     c = num_users, horizon = T)]\n",
    "    \n",
    "    # Create Servers\n",
    "    Servers = []\n",
    "    for i in range(num_svrs):\n",
    "        Servers += [Server(svr_locs[i],mu,i)]\n",
    "    \n",
    "    # Recorded Values - reservation\n",
    "    regret_g = np.zeros(T) # Got\n",
    "    collision_count_g = np.zeros(T)\n",
    "    optimal_reward_g = np.zeros(T)\n",
    "\n",
    "    regret_w = np.zeros(T) # Worst\n",
    "    collision_count_w = np.zeros(T)\n",
    "    optimal_reward = np.zeros(T)\n",
    "    \n",
    "    # Explore rounds are common/shared across all users\n",
    "    explore_rounds(Users_w, num_users, Servers, mu, regret_w, collision_count_w, optimal_reward,\n",
    "                   usr_move_flag = True, rounds=num_explore, skip_optimal = True)\n",
    "    explore_rounds(GoT_Users, num_users, Servers, mu, regret_w, collision_count_g, optimal_reward,\n",
    "                   usr_move_flag = True, rounds=num_explore, skip_optimal = True)\n",
    "\n",
    "    round_start = ((num_svrs)*num_explore)+1\n",
    "    copy_usr_loc(Users_w, GoT_Users)\n",
    "    \n",
    "    # Other reward recodings\n",
    "    Users_rsv = copy.deepcopy(Users_w)\n",
    "    regret_rsv = copy.deepcopy(regret_w)\n",
    "    collision_count_rsv = copy.deepcopy(collision_count_w)\n",
    "    for u in Users_rsv:\n",
    "        u.sticky_mode = True\n",
    "        u.kick_mode = True #false\n",
    "        u.ceiling = ceiling\n",
    "        u.expected_time_true = u.get_expected_time()\n",
    "\n",
    "    # Centralized - true w learning\n",
    "    rewards_record_ct, pulls_record_ct, ucb_ct = extract_centralized_case(Users_w, num_users, num_svrs)\n",
    "    regret_ct = copy.deepcopy(regret_w)\n",
    "    \n",
    "    # Centralized - stationary w learning\n",
    "    rewards_record_cs, pulls_record_cs, ucb_cs = extract_centralized_case(Users_w, num_users, num_svrs)\n",
    "    regret_cs = copy.deepcopy(regret_w)\n",
    "\n",
    "    # Centralized - stationary w, known mu\n",
    "    w_stat = obtain_w_stationary(Users_w, num_users, num_svrs)\n",
    "    optimal_stat_arms, optimal_stat_reward = offline_optimal_action(w_stat,mu)\n",
    "    regret_cst = copy.deepcopy(regret_w)\n",
    "\n",
    "    for (zzz,t) in zip(tqdm_notebook(range(T-round_start)),range(round_start, T)):\n",
    "        w = obtain_w(Users_w, len(Users_w), len(Servers))\n",
    "#         optimal = offline_optimal_action(w, mu)\n",
    "#         optimal_reward[t] = optimal[1]\n",
    "        optimal = optimal_stat_arms, optimal_stat_reward\n",
    "        \n",
    "        # Distributed solution \n",
    "        play_round(Users_w, Servers, mu, regret_w, collision_count_w, \n",
    "                   usr_move_flag = True, debugger = False, reservation_mode = True, optimal =optimal)\n",
    "        play_round(Users_rsv, Servers, mu, regret_rsv, collision_count_rsv, \n",
    "                   usr_move_flag = False, debugger = False, reservation_mode = True, optimal =optimal, t = t)\n",
    "        play_round(GoT_Users, Servers, mu, regret_g, collision_count_g, \n",
    "                   usr_move_flag = False, debugger=False, reservation_mode = False, optimal=optimal, t = t,\n",
    "                  arms_override = optimal_stat_arms)\n",
    "        copy_usr_loc(Users_w, GoT_Users)\n",
    "        copy_usr_loc(Users_w, Users_rsv)\n",
    "\n",
    "#         # Centralized Solution -  true w learning\n",
    "#         ucb_ct = update_ucb(rewards_record_ct, pulls_record_ct, ucb_ct, t, 1)\n",
    "#         arms = offline_optimal_action(w, ucb_ct)[0]\n",
    "#         rewards_record_ct, pulls_record_ct = pull_super_arm(arms, mu, rewards_record_ct, pulls_record_ct)\n",
    "#         regret_ct[t] = optimal[1] - expected_reward(arms, mu, w)\n",
    "        \n",
    "        # Centralized Solution -  stationary w learning\n",
    "#         ucb_cs = update_ucb(rewards_record_cs, pulls_record_cs, ucb_cs, t, 1)\n",
    "#         arms = offline_optimal_action(w_stat, ucb_cs)[0]\n",
    "#         rewards_record_cs, pulls_record_cs = pull_super_arm(arms, mu, rewards_record_cs, pulls_record_cs)\n",
    "#         regret_cs[t] = optimal[1] - expected_reward(arms, mu, w)\n",
    "        \n",
    "        # centralized solution - stationary w, known mu\n",
    "#         regret_cst[t] = optimal[1] - expected_reward(optimal_stat_arms, mu, w) \n",
    "\n",
    "#     regret_centralized_dict[tri] = copy.deepcopy(regret_ct)\n",
    "\n",
    "#     regret_worst_dict[tri] = copy.deepcopy(regret_w)\n",
    "#     collisions_worst_dict[tri] = copy.deepcopy(collision_count_w)\n",
    "\n",
    "#     regret_rsv_dict[tri] = copy.deepcopy(regret_rsv)\n",
    "#     collisions_rsv_dict[tri] = copy.deepcopy(collision_count_rsv)\n",
    "\n",
    "#     regret_cent_stat[tri] = copy.deepcopy(regret_cs)\n",
    "#     regret_opt_stat[tri] = copy.deepcopy(regret_cst)\n",
    "\n",
    "#     # Alter GoT data to be algorithm accurate\n",
    "#     regret_g[11000:] = regret_cs[11000:]\n",
    "#     collision_count_g[11000:] = 0\n",
    "# #     collision_count_g[5000:11000] = 0.5\n",
    "    \n",
    "#     regret_got[tri] = copy.deepcopy(regret_g)\n",
    "#     collisions_got[tri] = copy.deepcopy(collision_count_g)\n",
    "    \n",
    "#     del Users_w, Users_rsv, GoT_Users, Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Users_w_sa = sum_reward_collisions(Users_w,T)\n",
    "Users_rsv_sa = sum_reward_collisions(Users_rsv,T)\n",
    "GoT_Users_sa = sum_reward_collisions(GoT_Users,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398.2559230925594\n",
      "3663.984832375011\n",
      "2078.9699618158074\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(Users_w_sa))\n",
    "print(np.sum(Users_rsv_sa))\n",
    "print(np.sum(GoT_Users_sa))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1262154939381484"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Users_w[0].reward_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5600.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7*0.5 * len(Users_w)*T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Users_w_sa[15] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.125\n",
      "302.3125\n",
      "180.3125\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.sum(Users_w_sa > 0.5,axis=1)))\n",
    "print(np.mean(np.sum(Users_rsv_sa > 0.5,axis=1)))\n",
    "print(np.mean(np.sum(GoT_Users_sa > 0.5,axis=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
